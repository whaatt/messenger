{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analysis\n",
    "**Note:** Ensure you have completed the `Setup.ipynb` notebook.\n",
    "\n",
    "Now is also a good time to look through `schema.py` and familiarize yourself with the fields on each\n",
    "DB model (if you're so inclined, although this notebook will hold your hand). You might also find\n",
    "knowledge of the [Peewee](http://docs.peewee-orm.com) library for Python helpful.\n",
    "\n",
    "As before, replace the `folder_name` below with one you've already set up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# NOTE: Please change the folder name in `constants.py`.\n",
    "from constants import folder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from schema import get_chat_db\n",
    "\n",
    "ChatDB = get_chat_db(folder_name)\n",
    "ChatDB.open()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most Common Words\n",
    "A natural starting place for analysis is figuring out the most common words in our chat (excluding\n",
    "particles like `the` and `a`). We also would like to see the breakdown of common words by\n",
    "individual.\n",
    "\n",
    "First, let's use `nltk` to get a list of particle words (known more commonly as _stopwords_) for our\n",
    "chat language (which I'm assuming is English here, but please change it as you need to)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also build an auxiliary table around word usage (based on the full-text search index for\n",
    "messages):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MessageIndex = ChatDB.MessageIndex\n",
    "# Peewee has functionality for VocabModel, but for some reason does not actually run a CREATE.\n",
    "MessageIndex.raw(\n",
    "    \"CREATE VIRTUAL TABLE IF NOT EXISTS messageindex_v USING fts5vocab(messageindex, instance);\"\n",
    ").execute()\n",
    "MessageVocab = MessageIndex.VocabModel(table_type=\"instance\")\n",
    "MessageVocab._meta.remove_field(\"cnt\")  # Not present for VocabModel type `instance`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If that didn't make a ton of sense, and you're still curious, just know the following:\n",
    "- The `Message` table is referenced by the `MessageIndex` such that\n",
    "  `Message.id == MessageIndex.rowid` and `Message.content == MessageIndex.content` (the content is\n",
    "  duplicated, but `MessageIndex` uses efficient text indexing to allow full-text querying over its\n",
    "  `content` field).\n",
    "- `MessageVocab` is a table consisting of columns `term` and `doc` (and other stuff, but it's not\n",
    "  super relevant to us). For every occurrence of a word in a particular `MessageIndex` entry, there\n",
    "  is an entry in `MessageVocab` where `term` equals the word and `doc` equals the `rowid` from\n",
    "  `MessageIndex`.\n",
    "    - If we had values `banana tomato` and `carrot tomato` as the entries in `MessageIndex`, then\n",
    "      `MessageVocab` would have entries `banana, 0`, `tomato, 0`, `tomato, 1`, `carrot, 1`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**TLDR:** Now let's figure out the top 10 words in our chat. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from peewee import fn, SQL\n",
    "\n",
    "def print_top_ten(results, filter_fn=None):\n",
    "    # Collect results and filter stopwords.\n",
    "    counts = [(result.term, result.count) for result in results]\n",
    "    counts = list(filter(lambda x: x[0] not in stop_words, counts))\n",
    "\n",
    "    if filter_fn is not None:\n",
    "        counts = list(filter(filter_fn, counts))\n",
    "    \n",
    "    # Print top 10 results.\n",
    "    for word, count in counts[:10]:\n",
    "        print(f\"{word} ({count})\")\n",
    "        \n",
    "# Build query (words ordered by count, descending).\n",
    "results = (\n",
    "    MessageVocab.select(MessageVocab.term, fn.COUNT(MessageVocab.rowid).alias(\"count\"))\n",
    "    .group_by(MessageVocab.term)\n",
    "    .order_by(SQL(\"count\").desc())\n",
    ")\n",
    "\n",
    "print_top_ten(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's pretty interesting, but if your chat is anything like the ones I tested this on, you still\n",
    "might have some tiny filler words. Let's try that again, but limit words to 5 characters or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_top_ten(results, filter_fn=lambda x: len(x[0]) > 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now let's print the top ten non-stopwords by user:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Message = ChatDB.Message\n",
    "users = ChatDB.User.select()\n",
    "\n",
    "for user in users:\n",
    "    results = (\n",
    "        MessageVocab.select(MessageVocab.term, fn.COUNT(MessageVocab.rowid).alias(\"count\"))\n",
    "        .join(Message, on=(Message.id == MessageVocab.doc))\n",
    "        .where(Message.sender == user)\n",
    "        .group_by(MessageVocab.term)\n",
    "        .order_by(SQL(\"count\").desc())\n",
    "    )\n",
    "    \n",
    "    print(f\"User: {user}\")\n",
    "    print_top_ten(results)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Surprised?** Don't remember using `lmao` or whatever so many times? Use the function below to list\n",
    "the messages containing a particular word..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_messages_with(word):\n",
    "    results = (\n",
    "        Message.select(Message.id, Message.sender, Message.content)\n",
    "        .distinct()  # Note the important thing: this is distinct on id.\n",
    "        .join(MessageVocab, on=(Message.id == MessageVocab.doc))\n",
    "        .where(MessageVocab.term == word)\n",
    "    )\n",
    "\n",
    "    return [(result.sender.name, result.content) for result in results]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_messages_with('zuckerberg')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
